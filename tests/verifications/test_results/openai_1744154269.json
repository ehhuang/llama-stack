{
  "created": 1744154276.309212,
  "duration": 1.107724905014038,
  "exitcode": 1,
  "root": "/Users/erichuang/projects/llama-stack",
  "environment": {},
  "summary": {
    "error": 83,
    "total": 83,
    "collected": 83
  },
  "collectors": [
    {
      "nodeid": "",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py",
          "type": "Module"
        }
      ]
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 138
        }
      ]
    }
  ],
  "tests": [
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0006063750479370356,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1066f3b80>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026558293029665947,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00041820795740932226,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067cfe80>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002750830026343465,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003626249963417649,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067d5540>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00018108298536390066,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00021775008644908667,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067cd330>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025308295153081417,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026174995582550764,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067d6fb0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00019862502813339233,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00022054195869714022,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067ce860>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002409159205853939,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002621669555082917,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067d6f50>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0001611249754205346,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o-mini]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00020725000649690628,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067cdc90>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022341706790030003,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00022674992214888334,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e0310>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002440840471535921,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025754200760275126,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067ccb50>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000282999943010509,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00028849998489022255,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e2e60>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026454206090420485,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026812509167939425,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670f8e0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024104095064103603,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002595410915091634,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e1b70>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002335000317543745,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026241689920425415,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1066f3df0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002467499580234289,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002603749744594097,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10664dfc0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00021095795091241598,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o-mini]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00022962503135204315,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067cdea0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023650005459785461,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025337503757327795,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e0820>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00020995899103581905,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023791706189513206,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670d390>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00021612504497170448,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002458749804645777,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e23e0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002383749233558774,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024295796174556017,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670e2c0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002424169797450304,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002542909933254123,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e3a00>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00021079194266349077,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002358750207349658,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670e260>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023737503215670586,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002472080523148179,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e1210>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0001932079903781414,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o-mini]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003057920839637518,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670e5c0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025525002274662256,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025233300402760506,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e1b40>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025579193606972694,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002405830891802907,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067cc850>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023187510669231415,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00022974994499236345,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e2260>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022000004537403584,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024695799220353365,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x106702e90>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002448330633342266,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023074995260685682,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e3340>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002673330018296838,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026291701942682266,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10664e830>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022549997083842754,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023912498727440834,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067cda80>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002532079815864563,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o-mini]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023179198615252972,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e1600>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00027070799842476845,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002952920040115714,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1066d51e0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024045794270932674,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025054195430129766,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e1570>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00020725000649690628,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002484170254319906,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10664d600>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00021012499928474426,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00030649988912045956,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e23e0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023204204626381397,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003703329712152481,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067d6b00>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00019658298697322607,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o-mini]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002328749978914857,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e3f10>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00014579191338270903,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00019370904192328453,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067cc9d0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000167124904692173,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.000282666995190084,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670ffa0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002443749690428376,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00027304200921207666,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067ce530>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002806659322232008,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00028083391953259706,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670d3c0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00028191693127155304,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024399999529123306,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067cefe0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026999996043741703,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o-mini]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002569579519331455,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670c1f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002211659448221326,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.000216333894059062,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10680e440>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022833398543298244,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002446250291541219,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670d5d0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0003043330507352948,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003262499812990427,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10680d0f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026233296375721693,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023300002794712782,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670c490>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002502499846741557,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002428750740364194,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067d6170>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002614170080050826,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002649170346558094,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670e920>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000326832989230752,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024645798839628696,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x106703b50>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026145903393626213,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o-mini]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023745896760374308,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1068c1030>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023579096887260675,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002396659692749381,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067cd8a0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002518750261515379,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002475420478731394,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1068c2470>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00029070896562188864,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00031879195012152195,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067cf6a0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002224160125479102,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024500000290572643,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e3310>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00016649998724460602,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00020345905795693398,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1068c1ff0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026875000912696123,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00031870801467448473,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670ebc0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002550410572439432,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002460000105202198,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e3b50>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022966694086790085,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o-mini]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025616702623665333,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1068c0d00>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002401249948889017,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024862505961209536,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670fa30>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024812493938952684,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026037509087473154,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10664e740>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022237503435462713,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024099997244775295,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1066d6710>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026366603560745716,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002823329996317625,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1068c3280>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022383290342986584,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023212505038827658,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1066f08e0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002971250796690583,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026074994821101427,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1068c0a00>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024704099632799625,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003647090634331107,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670e8f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00021841691341251135,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o-mini]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023574999067932367,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1066d7e80>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024391699116677046,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003659160574898124,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x106808d90>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023754197172820568,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002460830146446824,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1066d7310>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024329102598130703,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002376670017838478,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10680a8c0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.004673875053413212,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0030857919482514262,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10664df00>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0012707909336313605,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002817079657688737,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x106809540>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00029891706071794033,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024687498807907104,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e3d30>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024737499188631773,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002747499383985996,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067d47c0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026750005781650543,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o-mini]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026404205709695816,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1066f39d0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023404194507747889,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024449999909847975,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067d65f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023787503596395254,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023358408361673355,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670c790>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023233401589095592,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024066597688943148,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067d7a00>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023458292707800865,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002331659197807312,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10670e470>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024149997625499964,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023779203183948994,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1068c81f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022187503054738045,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023804197553545237,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10680b4f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00021266692783683538,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o-mini]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023620901629328728,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1067e0550>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023370806593447924,
        "outcome": "passed"
      }
    }
  ]
}