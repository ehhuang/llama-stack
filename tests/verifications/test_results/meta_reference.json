{
  "created": 1744913887.9499788,
  "duration": 43.217848777770996,
  "exitcode": 1,
  "root": "/Users/erichuang/projects/llama-stack",
  "environment": {},
  "summary": {
    "failed": 28,
    "total": 28,
    "collected": 28
  },
  "collectors": [
    {
      "nodeid": "",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py",
          "type": "Module"
        }
      ]
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]",
          "type": "Function",
          "lineno": 114
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]",
          "type": "Function",
          "lineno": 114
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
          "type": "Function",
          "lineno": 157
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]",
          "type": "Function",
          "lineno": 181
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]",
          "type": "Function",
          "lineno": 181
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]",
          "type": "Function",
          "lineno": 204
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]",
          "type": "Function",
          "lineno": 204
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
          "type": "Function",
          "lineno": 226
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
          "type": "Function",
          "lineno": 250
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
          "type": "Function",
          "lineno": 278
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
          "type": "Function",
          "lineno": 302
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
          "type": "Function",
          "lineno": 329
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
          "type": "Function",
          "lineno": 352
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]",
          "type": "Function",
          "lineno": 380
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]",
          "type": "Function",
          "lineno": 380
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]",
          "type": "Function",
          "lineno": 380
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]",
          "type": "Function",
          "lineno": 380
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]",
          "type": "Function",
          "lineno": 380
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]",
          "type": "Function",
          "lineno": 471
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]",
          "type": "Function",
          "lineno": 471
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]",
          "type": "Function",
          "lineno": 471
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]",
          "type": "Function",
          "lineno": 471
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]",
          "type": "Function",
          "lineno": 471
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-False]",
          "type": "Function",
          "lineno": 554
        },
        {
          "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-True]",
          "type": "Function",
          "lineno": 554
        }
      ]
    }
  ],
  "tests": [
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]",
      "lineno": 95,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-earth",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "earth"
      },
      "setup": {
        "duration": 0.5453532499996072,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.4346592909987521,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 106,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x107772920>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]>>\nopenai_client = <openai.OpenAI object at 0x107772920>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'earth', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}]}, 'output': 'Earth'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:106: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x107772920>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.00027475000024423935,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]",
      "lineno": 95,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "saturn"
      },
      "setup": {
        "duration": 0.007205541998700937,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.3720934159991884,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 106,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1152f93c0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]>>\nopenai_client = <openai.OpenAI object at 0x1152f93c0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'saturn', 'input': {'messages': [{'content': 'Which planet has rings around it with a name starting with letter S?', 'role': 'user'}]}, 'output': 'Saturn'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:106: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1152f93c0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002617499994812533,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]",
      "lineno": 114,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-earth",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "earth"
      },
      "setup": {
        "duration": 0.007485124999220716,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.384115250000832,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 125,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x115424190>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]>>\nopenai_client = <openai.OpenAI object at 0x115424190>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'earth', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}]}, 'output': 'Earth'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:125: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x115424190>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.00024741600100242067,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]",
      "lineno": 114,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "saturn"
      },
      "setup": {
        "duration": 0.0072404579987050965,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.259584667001036,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 125,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1154ae110>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]>>\nopenai_client = <openai.OpenAI object at 0x1154ae110>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'saturn', 'input': {'messages': [{'content': 'Which planet has rings around it with a name starting with letter S?', 'role': 'user'}]}, 'output': 'Saturn'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:125: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1154ae110>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.00028862500039394945,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
      "lineno": 138,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "case0"
      },
      "setup": {
        "duration": 0.008278833000076702,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.2675952080007846,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 149,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x107f55a80>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x107f55a80>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': [{'text': 'What is in this image?', 'type': 'text'}, {'image_url': {...}, 'type': 'image_url'}], 'role': 'user'}]}, 'output': 'llama'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_image\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_image(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:149: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x107f55a80>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.00023920800049381796,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
      "lineno": 157,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "case0"
      },
      "setup": {
        "duration": 0.007464584001354524,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.309593167001367,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 168,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1151822c0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x1151822c0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': [{'text': 'What is in this image?', 'type': 'text'}, {'image_url': {...}, 'type': 'image_url'}], 'role': 'user'}]}, 'output': 'llama'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_image\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_image(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:168: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1151822c0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002605420013424009,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]",
      "lineno": 181,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "calendar"
      },
      "setup": {
        "duration": 0.00693962500008638,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.3343045830006304,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 192,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x107feba90>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]>>\nopenai_client = <openai.OpenAI object at 0x107feba90>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:192: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x107feba90>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002642499985086033,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]",
      "lineno": 181,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-math",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "math"
      },
      "setup": {
        "duration": 0.008520332999978564,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.3746494580009312,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 192,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x11518e860>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]>>\nopenai_client = <openai.OpenAI object at 0x11518e860>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:192: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11518e860>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002572089997556759,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]",
      "lineno": 204,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "calendar"
      },
      "setup": {
        "duration": 0.007074459001159994,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.358733500001108,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 215,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x11529c460>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]>>\nopenai_client = <openai.OpenAI object at 0x11529c460>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:215: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11529c460>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002741250009421492,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]",
      "lineno": 204,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-math",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "math"
      },
      "setup": {
        "duration": 0.0073204159998567775,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.390802709000127,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 215,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x11532bac0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]>>\nopenai_client = <openai.OpenAI object at 0x11532bac0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:215: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11532bac0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.00023887499992270023,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
      "lineno": 226,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "case0"
      },
      "setup": {
        "duration": 0.007100625000020955,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.247846207999828,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 237,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1154a2e30>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x1154a2e30>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:237: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1154a2e30>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002485410004737787,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
      "lineno": 250,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "case0"
      },
      "setup": {
        "duration": 0.006795833998694434,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.328455999999278,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 261,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1077cf880>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x1077cf880>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:261: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1077cf880>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002787080011330545,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
      "lineno": 278,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "case0"
      },
      "setup": {
        "duration": 0.007075333000102546,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.324009792000652,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 289,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1150638b0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x1150638b0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:289: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1150638b0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002546250016166596,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
      "lineno": 302,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "case0"
      },
      "setup": {
        "duration": 0.007278958000824787,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.408216583000467,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 313,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x107b06c80>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x107b06c80>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:313: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x107b06c80>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.00025970799833885394,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
      "lineno": 329,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "case0"
      },
      "setup": {
        "duration": 0.007451333000062732,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.3141664159993525,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 340,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x10784cb50>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x10784cb50>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:340: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x10784cb50>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002424159993097419,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
      "lineno": 352,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "case0"
      },
      "setup": {
        "duration": 0.00751124999987951,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.2616192919995228,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 363,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1076368f0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x1076368f0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:363: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1076368f0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002505829997971887,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]",
      "lineno": 380,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "text_then_weather_tool"
      },
      "setup": {
        "duration": 0.007525166000050376,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.4039755829999194,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 421,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x11529dde0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x11529dde0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n>           response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:421: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11529dde0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002504169988242211,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]",
      "lineno": 380,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "weather_tool_then_text"
      },
      "setup": {
        "duration": 0.007357665999734309,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.2483259589989757,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 421,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1076cace0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x1076cace0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n>           response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:421: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1076cace0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0003206670007784851,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]",
      "lineno": 380,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "add_product_tool"
      },
      "setup": {
        "duration": 0.006801833000281476,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.1911935830012226,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 421,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x115426ce0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x115426ce0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n>           response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:421: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x115426ce0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.00025216700123564806,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]",
      "lineno": 380,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "get_then_create_event_tool"
      },
      "setup": {
        "duration": 0.006625624999287538,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.20892883399938,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 421,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1154210c0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x1154210c0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n>           response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:421: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1154210c0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.00029637500119861215,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]",
      "lineno": 380,
      "outcome": "failed",
      "keywords": [
        "test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "compare_monthly_expense_tool"
      },
      "setup": {
        "duration": 0.007042165998427663,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.322087915999873,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 421,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1153f3ca0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x1153f3ca0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n>           response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:421: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1153f3ca0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002777089994197013,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]",
      "lineno": 471,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "text_then_weather_tool"
      },
      "setup": {
        "duration": 0.007169375001467415,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.4577338330000202,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 498,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x107770b20>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x107770b20>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n>           stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:498: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x107770b20>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002602499989734497,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]",
      "lineno": 471,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "weather_tool_then_text"
      },
      "setup": {
        "duration": 0.007024041999102337,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.3548274579989084,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 498,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x115061990>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x115061990>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n>           stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:498: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x115061990>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0003373749987076735,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]",
      "lineno": 471,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "add_product_tool"
      },
      "setup": {
        "duration": 0.012372082999718259,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.369784709000669,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 498,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1151f6320>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x1151f6320>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n>           stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:498: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1151f6320>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002671669990377268,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]",
      "lineno": 471,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "get_then_create_event_tool"
      },
      "setup": {
        "duration": 0.008659000000989181,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.446240041999772,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 498,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x1152c2350>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x1152c2350>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n>           stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:498: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1152c2350>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002721660002862336,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]",
      "lineno": 471,
      "outcome": "failed",
      "keywords": [
        "test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "compare_monthly_expense_tool"
      },
      "setup": {
        "duration": 0.007061292000798858,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.3421519999992597,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 498,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x107cf1ea0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x107cf1ea0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n>           stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:498: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x107cf1ea0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002517499997338746,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-False]",
      "lineno": 554,
      "outcome": "failed",
      "keywords": [
        "test_chat_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-False]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-False",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "False"
      },
      "setup": {
        "duration": 0.012158083000031183,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.4666849580007693,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 586,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x107dd2e00>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-False]>>\nopenai_client = <openai.OpenAI object at 0x107dd2e00>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\nmulti_image_data = ['data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGC...6pH9jaTzNv7vfRRXzubfxj9f8Pv8AkTz/AMX/ALbEz5Ly38lfMk/5Z/u64PxhqEZh+z/6rzvn2UUV5EvgPuzy/wAc6p5dt5ccibJpNkkdFFFec27mZ//Z']\nstream = False\n\n    @pytest.mark.parametrize(\"stream\", [False, True])\n    def test_chat_multiple_images(request, openai_client, model, provider, verification_config, multi_image_data, stream):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages_turn1 = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": multi_image_data[0],\n                        },\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": multi_image_data[1],\n                        },\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"What furniture is in the first image that is not in the second image?\",\n                    },\n                ],\n            },\n        ]\n    \n        # First API call\n>       response1 = openai_client.chat.completions.create(\n            model=model,\n            messages=messages_turn1,\n            stream=stream,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:586: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x107dd2e00>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0002765420013020048,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-True]",
      "lineno": 554,
      "outcome": "failed",
      "keywords": [
        "test_chat_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-True]",
        "parametrize",
        "pytestmark",
        "meta-llama/Llama-4-Scout-17B-16E-Instruct-True",
        "test_chat_completion.py",
        "openai_api",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "metadata": {
        "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "case_id": "True"
      },
      "setup": {
        "duration": 0.01194374999977299,
        "outcome": "passed"
      },
      "call": {
        "duration": 1.3468280830002186,
        "outcome": "failed",
        "crash": {
          "path": "/Users/erichuang/.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
          "lineno": 989,
          "message": "openai.APIConnectionError: Connection error."
        },
        "traceback": [
          {
            "path": "tests/verifications/openai_api/test_chat_completion.py",
            "lineno": 586,
            "message": ""
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py",
            "lineno": 279,
            "message": "in wrapper"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py",
            "lineno": 914,
            "message": "in create"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1242,
            "message": "in post"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 919,
            "message": "in request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 979,
            "message": "in _request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 1057,
            "message": "in _retry_request"
          },
          {
            "path": "../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py",
            "lineno": 989,
            "message": "APIConnectionError"
          }
        ],
        "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:250: in handle_request\n    resp = self._pool.handle_request(req)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256: in handle_request\n    raise exc from None\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236: in handle_request\n    response = connection.handle_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:101: in handle_request\n    raise exc\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:78: in handle_request\n    stream = self._connect(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_sync/connection.py:124: in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_backends/sync.py:207: in connect_tcp\n    with map_exceptions(exc_map):\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpcore/_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.OpenAI object at 0x107bd0250>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n>           response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:955: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:914: in send\n    response = self._send_handling_auth(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:942: in _send_handling_auth\n    response = self._send_handling_redirects(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:979: in _send_handling_redirects\n    response = self._send_single_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_client.py:1014: in _send_single_request\n    response = transport.handle_request(request)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:249: in handle_request\n    with map_httpcore_exceptions():\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/contextlib.py:153: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError: [Errno 61] Connection refused\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/httpx/_transports/default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-True]>>\nopenai_client = <openai.OpenAI object at 0x107bd0250>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'meta_reference'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\nmulti_image_data = ['data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGC...6pH9jaTzNv7vfRRXzubfxj9f8Pv8AkTz/AMX/ALbEz5Ly38lfMk/5Z/u64PxhqEZh+z/6rzvn2UUV5EvgPuzy/wAc6p5dt5ccibJpNkkdFFFec27mZ//Z']\nstream = True\n\n    @pytest.mark.parametrize(\"stream\", [False, True])\n    def test_chat_multiple_images(request, openai_client, model, provider, verification_config, multi_image_data, stream):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages_turn1 = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": multi_image_data[0],\n                        },\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": multi_image_data[1],\n                        },\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"What furniture is in the first image that is not in the second image?\",\n                    },\n                ],\n            },\n        ]\n    \n        # First API call\n>       response1 = openai_client.chat.completions.create(\n            model=model,\n            messages=messages_turn1,\n            stream=stream,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:586: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:979: in _request\n    return self._retry_request(\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x107bd0250>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n>           raise APIConnectionError(request=request) from err\nE           openai.APIConnectionError: Connection error.\n\n../../.cache/uv/archive-v0/OInMrHyP7p3zvGOl93HK1/lib/python3.10/site-packages/openai/_base_client.py:989: APIConnectionError"
      },
      "teardown": {
        "duration": 0.0005524169991986128,
        "outcome": "passed"
      }
    }
  ],
  "run_timestamp": 1744913836
}
