{
  "created": 1744153652.389766,
  "duration": 1.0027689933776855,
  "exitcode": 1,
  "root": "/Users/erichuang/projects/llama-stack",
  "environment": {},
  "summary": {
    "error": 83,
    "total": 83,
    "collected": 83
  },
  "collectors": [
    {
      "nodeid": "",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py",
          "type": "Module"
        }
      ]
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 138
        }
      ]
    }
  ],
  "tests": [
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0005435410421341658,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1107f3c10>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00027041707653552294,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025445909705013037,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cf130>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025358295533806086,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024404190480709076,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d19f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022854108829051256,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025929196272045374,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cee90>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022666598670184612,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024029100313782692,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d2aa0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000363041996024549,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026770797558128834,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108ceec0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00015154201537370682,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00021183292847126722,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d0130>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002797910710796714,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o-mini]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.000268499949015677,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cd6f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00027054210659116507,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026875000912696123,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d2620>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002601250307634473,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002472080523148179,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110752bc0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024375005159527063,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.000236041028983891,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d0130>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026995805092155933,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025566702242940664,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1107f2fb0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024737499188631773,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.000251457910053432,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d0af0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023408292327076197,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024570804089307785,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110800070>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0003100830363109708,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024358299560844898,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d3f10>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023312494158744812,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o-mini]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026833300944417715,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110801330>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002608749782666564,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002709999680519104,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d2620>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00021666602697223425,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00022729195188730955,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cfb20>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002559999702498317,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025183300022035837,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110958520>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002044580178335309,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023937493097037077,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cce20>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002349159913137555,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002472499618306756,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11095a3b0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002531249774619937,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002446250291541219,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cdd50>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002055830555036664,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00021883309818804264,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d1c60>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00021995906718075275,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o-mini]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024070893414318562,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cc8b0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022879207972437143,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025220797397196293,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109cfd60>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002341249492019415,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002594159450381994,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d1cf0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023208302445709705,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024491699878126383,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109d31c0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002416670322418213,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023337500169873238,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110801e70>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000250832992605865,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002501669805496931,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11095aa70>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023583299480378628,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023333297576755285,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108024d0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00019641593098640442,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00022325001191347837,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109589d0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024199998006224632,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o-mini]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025337503757327795,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cc520>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00017150002531707287,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00021295901387929916,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1107d7910>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00013812503311783075,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00019395805429667234,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1107530a0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026587501633912325,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00032541609834879637,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1107f3a60>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002488750033080578,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.000328624970279634,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108ce950>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026558293029665947,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003583329962566495,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1107f36a0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00027050008065998554,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o-mini]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.000269667012616992,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110801900>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002453339984640479,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.000260957982391119,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110958e80>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002504579024389386,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002673339331522584,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108ccb80>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025633396580815315,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002637919969856739,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109587f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022558297496289015,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00022566691040992737,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11080cd60>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002497079549357295,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023875001352280378,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110958610>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00021587498486042023,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o-mini]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002484170254319906,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108ce380>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022749998606741428,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.000255499966442585,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d25f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002644170308485627,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002657909644767642,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cf520>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025529100093990564,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024891598150134087,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d1600>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024216598831117153,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002845000708475709,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cc7f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00021162501070648432,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00022866600193083286,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d2500>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002475420478731394,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024516601115465164,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109cfe50>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022737507242709398,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024533295072615147,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d2260>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023658305872231722,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o-mini]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002406660933047533,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109cc160>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002794580068439245,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025649997405707836,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110752980>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024641596246510744,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002329589333385229,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109cce20>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002383330138400197,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023837503977119923,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11080caf0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025116605684161186,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002633329713717103,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d1840>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002538328990340233,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00027029099874198437,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109ccfa0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00030212500132620335,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00032362493220716715,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109cb8e0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025045801885426044,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002537500113248825,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11080d270>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002600420266389847,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o-mini]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002720000920817256,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110958a00>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024495902471244335,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026012491434812546,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109cc0d0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002436250215396285,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025116594042629004,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1107d77c0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023324997164309025,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024208310060203075,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d1360>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022870802786201239,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002686670050024986,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cfa60>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002518750261515379,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002662499900907278,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1107f16f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002584999892860651,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025395804550498724,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cfb20>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024737499188631773,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002839580411091447,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109cc760>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002138330601155758,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o-mini]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00021879200357943773,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cf670>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002347499830648303,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025200005620718,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11080dae0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00020483299158513546,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002227500081062317,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cdab0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022841698955744505,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023312505800276995,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11080d360>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000250832992605865,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024099997244775295,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108ce350>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000225124997086823,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024874997325241566,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11080dab0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024816696532070637,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002375839976593852,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1107520b0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00022599997464567423,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024141697213053703,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11080ec80>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023633299861103296,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o-mini]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024037505500018597,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108d3a60>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023320899344980717,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024670897983014584,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11080fee0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002798750065267086,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024850002955645323,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1107d74f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024304201360791922,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002491249470040202,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1108cd1b0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023479200899600983,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023379200138151646,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109d0730>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002536249812692404,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025058304890990257,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1107f15d0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00029866595286875963,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002927089808508754,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1109cf7f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026345800142735243,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o-mini]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00031404197216033936,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11080c3d0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0004055409226566553,
        "outcome": "passed"
      }
    }
  ]
}
