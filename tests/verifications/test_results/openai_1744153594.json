{
  "created": 1744153605.5958269,
  "duration": 3.6922788619995117,
  "exitcode": 1,
  "root": "/Users/erichuang/projects/llama-stack",
  "environment": {},
  "summary": {
    "error": 83,
    "total": 83,
    "collected": 83
  },
  "collectors": [
    {
      "nodeid": "",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py",
          "type": "Module"
        }
      ]
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 40
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 60
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 75
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 95
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o-mini]",
          "type": "Function",
          "lineno": 117
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-3.3-70B-Instruct]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E-Instruct]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o]",
          "type": "Function",
          "lineno": 138
        },
        {
          "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o-mini]",
          "type": "Function",
          "lineno": 138
        }
      ]
    }
  ],
  "tests": [
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0007158750668168068,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1104f7df0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023354100994765759,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024870806373655796,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11044ada0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026279198937118053,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025866704527288675,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105fbb50>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002497909590601921,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026891601737588644,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105d2770>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002376670017838478,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003154580481350422,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105f9db0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000259542022831738,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002684589708223939,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105d1ae0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026491598691791296,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026691704988479614,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105f8e80>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00032016600016504526,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output0-gpt-4o-mini]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002942080609500408,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105d3580>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0003327080048620701,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002807079581543803,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11044a410>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000266083050519228,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025454198475927114,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110514be0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002767919795587659,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00035083398688584566,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1104f4dc0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002572089433670044,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002770000137388706,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110516620>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002796250628307462,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.000286665977910161,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105ddcf0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024558405857533216,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00037687504664063454,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105168c0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00027650000993162394,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002709999680519104,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105f9810>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025829195510596037,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_basic[input_output1-gpt-4o-mini]",
      "lineno": 25,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_basic[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00027475005481392145,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110515270>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002685829531401396,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00027358403895050287,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105fa980>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026470795273780823,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003904580371454358,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105179a0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002507079625502229,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00033083301968872547,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105faec0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00027058308478444815,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002794999163597822,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110516590>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002829160075634718,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002699160249903798,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105fb340>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025875004939734936,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026520900428295135,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110516530>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00027370802126824856,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00029145798180252314,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105d1960>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002525829477235675,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output0-gpt-4o-mini]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002530000638216734,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105162f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026000000070780516,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002731661079451442,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105f9030>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0006078750593587756,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003132499987259507,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1106ca0b0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002557080006226897,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025979208294302225,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105d0a00>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024179206229746342,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00036666705273091793,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105164a0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000262125045992434,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025862501934170723,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110515d50>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024245795793831348,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024524994660168886,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1104da800>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000212125014513731,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00023612508084625006,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105d3670>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002578330459073186,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_basic[input_output1-gpt-4o-mini]",
      "lineno": 40,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_basic[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00027137494180351496,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110503790>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023870891891419888,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002549169585108757,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110516f50>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0003570000408217311,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00027366692665964365,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1106c8070>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00026754208374768496,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003060409799218178,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110516740>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00027424993459135294,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0007330829976126552,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110500c40>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002625000197440386,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002673750277608633,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105144c0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002487089950591326,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_image[input_output0-gpt-4o-mini]",
      "lineno": 60,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_image[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025104102678596973,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1106c9f60>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002609589137136936,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002537090331315994,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110515c90>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024937500711530447,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002651660470291972,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1106cbd30>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000316707999445498,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00027383293490856886,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110501750>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00027733296155929565,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025525002274662256,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105deb30>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002900839317589998,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002774999011307955,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105f8280>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0004899590276181698,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_image[input_output0-gpt-4o-mini]",
      "lineno": 75,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_image[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003109999233856797,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105ded10>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002757089678198099,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00028608308639377356,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110503b80>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000251833931542933,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002514580264687538,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105dc8b0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002534170635044575,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003583339275792241,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105024a0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002559170825406909,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00034204195253551006,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105fbc70>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002817079657688737,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003628339618444443,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110501ba0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00027899991255253553,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00034374999813735485,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1106c9990>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00041912496089935303,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0004672079812735319,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110501f90>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0014628750504925847,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output0-gpt-4o-mini]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0009756250074133277,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110449f00>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00041104096453636885,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002666669897735119,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110503f10>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.011734333005733788,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.003439707914367318,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1104f41f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00029191700741648674,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00027287506964057684,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105f8550>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0004483340308070183,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0004937499761581421,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1106c8070>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024704192765057087,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0007308329222723842,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105fb340>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0003761668922379613,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002795410109683871,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1106ca080>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00045341707300394773,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00028891698457300663,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110515cc0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0003749999450519681,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_structured_output[input_output1-gpt-4o-mini]",
      "lineno": 95,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_structured_output[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0028782079461961985,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1104db5e0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.08032579196151346,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0007767499191686511,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105fb970>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00031625002156943083,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0008374579483643174,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1104da980>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002599590225145221,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002607089700177312,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105fb7f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.001050833030603826,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003100000321865082,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1104d9b10>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0008834169711917639,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0007071670843288302,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110516530>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025941606145352125,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024929107166826725,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1106c9cf0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002705409424379468,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025404104962944984,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110517ee0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002382919192314148,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output0-gpt-4o-mini]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024095794651657343,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1106c9ba0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002672090195119381,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-3.3-8B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-8B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024916697293519974,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1106e6e00>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002377500059083104,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002471249317750335,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105d0eb0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025466701481491327,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002586659975349903,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1106e5360>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002641670871526003,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026012491434812546,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110515750>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.000240249908529222,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00041370897088199854,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105fa8f0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002905830042436719,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output1-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0004300830187276006,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x11062a740>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00023799994960427284,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002435420174151659,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105d0d90>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00028366607148200274,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_streaming_structured_output[input_output1-gpt-4o-mini]",
      "lineno": 117,
      "outcome": "error",
      "keywords": [
        "test_chat_streaming_structured_output[input_output1-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output1-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00028008397202938795,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1104f5840>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024970900267362595,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-3.3-70B-Instruct]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-3.3-70B-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-3.3-70B-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00025049992837011814,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105179a0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00024208298418670893,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002572080120444298,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1104f5030>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025745807215571404,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E-Instruct]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Scout-17B-16E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Scout-17B-16E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026595802046358585,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110517eb0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002639580052345991,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0002699160249903798,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105dfd90>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00025208306033164263,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-Llama-4-Maverick-17B-128E-Instruct]",
        "parametrize",
        "pytestmark",
        "input_output0-Llama-4-Maverick-17B-128E-Instruct",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00024087494239211082,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105f8b80>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.0002704169601202011,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-gpt-4o]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.0003684170078486204,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x110629150>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00029020896181464195,
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/verifications/openai/test_chat_completion.py::test_chat_non_streaming_tool_calling[input_output0-gpt-4o-mini]",
      "lineno": 138,
      "outcome": "error",
      "keywords": [
        "test_chat_non_streaming_tool_calling[input_output0-gpt-4o-mini]",
        "parametrize",
        "pytestmark",
        "input_output0-gpt-4o-mini",
        "test_chat_completion.py",
        "openai",
        "verifications",
        "tests",
        "llama-stack",
        ""
      ],
      "setup": {
        "duration": 0.00026879191864281893,
        "outcome": "failed",
        "crash": {
          "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
          "lineno": 114,
          "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
        },
        "traceback": [
          {
            "path": "/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py",
            "lineno": 94,
            "message": ""
          },
          {
            "path": "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py",
            "lineno": 114,
            "message": "OpenAIError"
          }
        ],
        "longrepr": "base_url = 'https://api.openai.com/v1', api_key = None\n\n>   ???\n\n/Users/erichuang/projects/llama-stack/verifications/openai/fixtures/fixtures.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x1105d0910>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/openai/_client.py:114: OpenAIError"
      },
      "teardown": {
        "duration": 0.00029562495183199644,
        "outcome": "passed"
      }
    }
  ]
}
